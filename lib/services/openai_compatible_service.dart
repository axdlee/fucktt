import 'dart:convert';
import 'dart:developer';
import 'dart:async';
import 'package:dio/dio.dart';
import '../models/ai_provider_model.dart';
import 'ai_service_interface.dart';

/// OpenAIå…¼å®¹çš„AIæœåŠ¡å®ç°
/// æ”¯æŒOpenAIã€DeepSeekã€é€šä¹‰åƒé—®ç­‰å…¼å®¹OpenAI APIæ ¼å¼çš„æœåŠ¡
class OpenAICompatibleService implements AIService {
  final AIProviderModel _provider;
  final Dio _dio;

  OpenAICompatibleService(this._provider) : _dio = Dio() {
    _configureClient();
  }

  @override
  AIProviderModel get provider => _provider;

  void _configureClient() {
    _dio.options.baseUrl = _provider.baseUrl;
    _dio.options.headers.addAll({
      'Content-Type': 'application/json',
      'Authorization': 'Bearer ${_provider.apiKey}',
      ..._provider.headers,
    });
    
    // è®¾ç½®è¶…æ—¶æ—¶é—´
    _dio.options.connectTimeout = const Duration(seconds: 30);
    _dio.options.receiveTimeout = const Duration(seconds: 120);
    
    // æ·»åŠ æ‹¦æˆªå™¨ç”¨äºæ—¥å¿—å’Œé”™è¯¯å¤„ç†
    _dio.interceptors.add(LogInterceptor(
      requestBody: false, // é¿å…è®°å½•æ•æ„Ÿä¿¡æ¯
      responseBody: false,
    ));
  }

  @override
  Future<bool> checkAvailability() async {
    try {
      final response = await _dio.get('/models');
      return response.statusCode == 200;
    } catch (e) {
      return false;
    }
  }

  @override
  Future<AIResponse> chat({
    required String prompt,
    String? modelId,
    Map<String, dynamic>? parameters,
  }) async {
    try {
      final model = modelId ?? _getDefaultModel();
      final request = _buildChatRequest(prompt, model, parameters);
      
      // è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°è¯·æ±‚ä¿¡æ¯
      log('ğŸš€ AIè¯·æ±‚: ${_provider.name}');
      log('ğŸ¯ æ¨¡å‹: $model');
      log('ğŸ“¦ è¯·æ±‚URL: ${_provider.baseUrl}/chat/completions');
      
      final response = await _dio.post('/chat/completions', data: request);
      
      if (response.statusCode == 200) {
        final result = _parseResponse(response.data, model);
        log('ğŸ¯ å“åº”å†…å®¹: ${result.content}');
        log('âœ… AIå“åº”æˆåŠŸ: ${result.content.length}å­—ç¬¦');
        return result;
      } else {
        throw AIServiceException(
          'APIè¯·æ±‚å¤±è´¥: ${response.statusCode}',
          code: response.statusCode.toString(),
        );
      }
    } on DioException catch (e) {
      log('âŒ AIè¯·æ±‚å¤±è´¥: ${_handleDioError(e)}');
      if (e.response?.data != null) {
        log('âŒ é”™è¯¯å“åº”: ${e.response!.data}');
      }
      throw AIServiceException(
        _handleDioError(e),
        code: e.response?.statusCode.toString(),
      );
    } catch (e) {
      log('âŒ AIè¯·æ±‚å¼‚å¸¸: $e');
      throw AIServiceException('è¯·æ±‚å¤±è´¥: $e');
    }
  }

  @override
  Stream<String> chatStream({
    required String prompt,
    String? modelId,
    Map<String, dynamic>? parameters,
  }) async* {
    try {
      final model = modelId ?? _getDefaultModel();
      final request = _buildChatRequest(prompt, model, parameters);
      request['stream'] = true;
      
      final response = await _dio.post<ResponseBody>(
        '/chat/completions',
        data: request,
        options: Options(responseType: ResponseType.stream),
      );
      
      if (response.statusCode == 200 && response.data != null) {
        final stream = response.data!.stream.map((event) => event as List<int>);
        await for (final chunk in stream.transform(utf8.decoder)) {
          final lines = chunk.split('\n');
          for (final line in lines) {
            if (line.startsWith('data: ')) {
              final data = line.substring(6).trim();
              if (data == '[DONE]') return;
              
              try {
                final json = jsonDecode(data);
                final delta = json['choices']?[0]?['delta'];
                if (delta != null && delta['content'] != null) {
                  yield delta['content'] as String;
                }
              } catch (e) {
                // å¿½ç•¥è§£æé”™è¯¯çš„è¡Œ
                continue;
              }
            }
          }
        }
      }
    } on DioException catch (e) {
      throw AIServiceException(_handleDioError(e));
    } catch (e) {
      throw AIServiceException('æµå¼è¯·æ±‚å¤±è´¥: $e');
    }
  }

  @override
  Future<List<ModelConfig>> getAvailableModels() async {
    try {
      final response = await _dio.get('/models');
      
      if (response.statusCode == 200) {
        final data = response.data;
        if (data['data'] is List) {
          return (data['data'] as List).map((model) {
            return ModelConfig(
              modelId: model['id'] as String,
              displayName: model['id'] as String,
              description: model['object'] as String?,
            );
          }).toList();
        }
      }
      
      // å¦‚æœAPIä¸æ”¯æŒè·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›é¢„é…ç½®çš„æ¨¡å‹
      return _provider.supportedModels;
    } catch (e) {
      // è¿”å›é¢„é…ç½®çš„æ¨¡å‹ä½œä¸ºåå¤‡
      return _provider.supportedModels;
    }
  }

  @override
  Future<bool> validateConfiguration() async {
    try {
      // å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
      final testPrompt = 'æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤"è¿æ¥æˆåŠŸ"';
      final response = await chat(prompt: testPrompt);
      return response.success && response.content.isNotEmpty;
    } catch (e) {
      return false;
    }
  }

  String _getDefaultModel() {
    // ä¼˜å…ˆä½¿ç”¨é…ç½®çš„æ¨¡å‹
    if (_provider.supportedModels.isNotEmpty) {
      return _provider.supportedModels.first.modelId;
    }
    
    // æ ¹æ®æœåŠ¡å•†è¿”å›é»˜è®¤æ¨¡å‹
    final providerName = _provider.name.toLowerCase();
    final baseUrl = _provider.baseUrl.toLowerCase();
    
    // SiliconFlowæœåŠ¡è¯†åˆ«
    if (providerName.contains('siliconflow') || baseUrl.contains('siliconflow')) {
      return 'deepseek-ai/DeepSeek-V2.5';
    }
    
    switch (providerName) {
      case 'openai':
        return 'gpt-3.5-turbo';
      case 'deepseek':
        return 'deepseek-chat';
      case 'alibaba':
      case 'qwen':
        return 'qwen-turbo';
      case 'baidu':
        return 'ernie-bot-turbo';
      case 'anthropic':
        return 'claude-3-haiku-20240307';
      case 'google':
        return 'gemini-pro';
      default:
        // æ£€æŸ¥å…¶ä»–æœåŠ¡çš„URLæ ‡è¯†
        if (baseUrl.contains('openai')) {
          return 'gpt-3.5-turbo';
        } else if (baseUrl.contains('deepseek')) {
          return 'deepseek-chat';
        } else if (baseUrl.contains('anthropic')) {
          return 'claude-3-haiku-20240307';
        }
        
        // æœ€åçš„å…œåº•ç­–ç•¥ï¼šä¸è¿”å›"default"ï¼Œè€Œæ˜¯å¸¸è§æ¨¡å‹
        return 'gpt-3.5-turbo';
    }
  }

  Map<String, dynamic> _buildChatRequest(
    String prompt,
    String model,
    Map<String, dynamic>? parameters,
  ) {
    final messages = [
      ChatMessage(role: 'user', content: prompt).toJson(),
    ];

    final request = <String, dynamic>{
      'model': model,
      'messages': messages,
      'temperature': 0.7,
      'max_tokens': 2048,
    };

    // åº”ç”¨è‡ªå®šä¹‰å‚æ•°
    if (parameters != null) {
      request.addAll(parameters);
    }

    // åº”ç”¨æ¨¡å‹ç‰¹å®šé…ç½®
    final modelConfig = _provider.supportedModels
        .where((config) => config.modelId == model)
        .firstOrNull;
    
    if (modelConfig != null) {
      request['temperature'] = modelConfig.temperature;
      request['max_tokens'] = modelConfig.maxTokens;
      if (modelConfig.topP != null) {
        request['top_p'] = modelConfig.topP;
      }
      if (modelConfig.frequencyPenalty != null) {
        request['frequency_penalty'] = modelConfig.frequencyPenalty;
      }
      if (modelConfig.presencePenalty != null) {
        request['presence_penalty'] = modelConfig.presencePenalty;
      }
      request.addAll(modelConfig.parameters);
    }

    return request;
  }

  AIResponse _parseResponse(Map<String, dynamic> data, String model) {
    try {
      final choices = data['choices'] as List?;
      if (choices == null || choices.isEmpty) {
        throw AIServiceException('å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ');
      }

      final choice = choices.first;
      final message = choice['message'] as Map<String, dynamic>?;
      if (message == null) {
        throw AIServiceException('å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘messageå­—æ®µ');
      }

      final content = message['content'] as String? ?? '';
      final usage = data['usage'] as Map<String, dynamic>? ?? {};

      return AIResponse(
        content: content,
        modelId: model,
        usage: usage,
        timestamp: DateTime.now(),
        success: true,
        metadata: {
          'finishReason': choice['finish_reason'],
          'rawResponse': data,
        },
      );
    } catch (e) {
      throw AIServiceException('è§£æå“åº”å¤±è´¥: $e');
    }
  }

  String _handleDioError(DioException e) {
    switch (e.type) {
      case DioExceptionType.connectionTimeout:
        return 'è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥';
      case DioExceptionType.sendTimeout:
        return 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•';
      case DioExceptionType.receiveTimeout:
        return 'å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•';
      case DioExceptionType.badResponse:
        final statusCode = e.response?.statusCode;
        final errorData = e.response?.data;
        
        if (statusCode == 401) {
          return 'APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®';
        } else if (statusCode == 429) {
          return 'è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•';
        } else if (statusCode == 500) {
          return 'AIæœåŠ¡å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•';
        }
        
        if (errorData is Map && errorData.containsKey('error')) {
          final error = errorData['error'];
          if (error is Map && error.containsKey('message')) {
            return error['message'] as String;
          }
        }
        
        return 'è¯·æ±‚å¤±è´¥ (HTTP $statusCode)';
      case DioExceptionType.cancel:
        return 'è¯·æ±‚å·²å–æ¶ˆ';
      case DioExceptionType.unknown:
        return 'ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®';
      default:
        return 'æœªçŸ¥é”™è¯¯: ${e.message}';
    }
  }

  void dispose() {
    _dio.close();
  }
}

extension on List<ModelConfig> {
  ModelConfig? get firstOrNull => isEmpty ? null : first;
}